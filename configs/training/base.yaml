resume_from_ckpt: null              # resume training from pl checkpoint
pretrained_path: null
train_moe: false                    # train moe
trainer:
    max_epochs: 3
    accelerator: "auto"             # "cpu", "gpu", "tpu", "hpu", "auto"
    # strategy: 'ddp'   # Distributed training strategy
    # strategy: 'deepspeed_stage_2'   # Distributed training strategy
    accumulate_grad_batches: 1
    gradient_clip_val: 1  # 0.1
    log_every_n_steps: 5
    val_check_interval: 0.5        # check validation set 5 times during a training epoch
    # check_val_every_n_epoch: 2
    precision: null
    profiler: null
optimizer:
    _target_: torch.optim.AdamW     # Optimizer type
    lr: 5e-5                        # Learning rate
    weight_decay: 0.1               # Weight decay coefficient for regularization
scheduler:
    name: 'linear'
    num_training_steps: 12000
    num_warmup_steps: 200
